import mlx.core as mx
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
%matplotlib inline


def one_hot(t, num_classes):
    t_enc = mx.zeros((t.size, num_classes))
    for i,n in enumerate(t):
        t_enc[i, n] = 1
    return t_enc


words = open('names.txt', 'r').read().splitlines()


chars = sorted(list(set(''.join(words))))


stoi = {s:i+1 for i,s in enumerate(chars)}
stoi["."] = 0


itos = {i:s for s,i in stoi.items()}


N = mx.zeros((27, 27)).astype(mx.int32)


for w in words:
    chs = ['.'] + list(w) + ['.']
    mx.eval(N)
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        N[ix1, ix2] += 1


N


plt.figure(figsize=(16,16))
plt.imshow(N, cmap="Blues")
for i in range(27):
    for j in range(27):
        chstr = itos[i] + itos[j]
        plt.text(j, i, chstr, ha="center", va="bottom", color="gray")
        plt.text(j, i, N[i, j].item(), ha="center", va="top", color="gray")
plt.axis("off");


learning_rate = 2.0
batch_size = 1000
epochs = 10
num_classes = 27
key = mx.random.key(42)
loss = 0.0
W = mx.random.normal(shape=(num_classes,num_classes), dtype=mx.float32, key=key)
W


xs = []
ys = []
for w in words[:batch_size]:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        x, y = stoi[ch1], stoi[ch2]
        xs.append(x)
        ys.append(y)    

xs = mx.array(xs)
num = xs.size
ys = mx.array(ys)


(xs, xs.shape)


(ys, ys.shape)


def loss_fn(x, w):
    logits = x @ w
    counts = mx.exp(logits)
    probs = counts / counts.sum(axis=1, keepdims=True)
    return mx.negative(mx.mean(mx.log(probs[mx.arange(num), ys])))

grad_fn = mx.grad(loss_fn)


# for k in range(10):
X = one_hot(xs, num_classes).astype(mx.float32)
print(X.shape)
grad = grad_fn(X, W)
W = mx.subtract(W, mx.multiply(learning_rate, grad))
# mx.eval(W)
# loss = loss_fn(X, W)
# print("loss: ", loss)





xs, ys = [], []
for w in words:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        x, y = stoi[ch1], stoi[ch2]
        xs.append(x)
        ys.append(y)    

xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print('number of examples: ', num)

g = torch.Generator().manual_seed(42)
W = torch.randn((27, 27), generator=g, requires_grad=True)


print(W.shape)
for k in range(2):
    xenc = F.one_hot(xs, num_classes=num_classes).float()
    print(xenc.shape)
    logits = xenc @ W
    print(logits.shape)
    counts = logits.exp()
    probs = counts / counts.sum(1, keepdims=True)
    print(probs.shape)
    loss = -probs[torch.arange(num), ys].log().mean()
    print(loss.shape)
    print(loss.item())

    W.grad = None
    loss.backward()
    W.data = W.data - 10 * W.grad



